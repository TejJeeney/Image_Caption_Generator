{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee94db-a404-4385-90a8-0b8f3ee8b050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "583fe1b1-4b30-44fa-9432-82264a0b3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file tokens_clean.txt and store the cleaned captions in a dictionary\n",
    "import json\n",
    "\n",
    "content = None\n",
    "\n",
    "with open (\"C:/Users/ASUS/Desktop/image caption generator/data/textFiles/tokens_clean.txt\", 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "json_acceptable_string = content.replace(\"'\", \"\\\"\")\n",
    "content = json.loads(json_acceptable_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9787eb4-08b8-44c1-a5fe-48f11a8b1167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53a99776-b5fe-47c7-80f6-2d2064e23625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words = 1955246\n"
     ]
    }
   ],
   "source": [
    "#Iterate over the captions word by word, and append each word to total_words\n",
    "total_words = []\n",
    "\n",
    "for key in content.keys():\n",
    "    for caption in content[key]:\n",
    "        for i in caption.split():\n",
    "            total_words.append(i)\n",
    "\n",
    "print(\"Total Words = %d\" %len(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5006fa4c-d727-48e3-8626-64edc9736008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words = 18079\n"
     ]
    }
   ],
   "source": [
    "# Compute the frequency of occurrence of each word, and store it in a dictionary of word-freq\n",
    "import collections\n",
    "\n",
    "counter = collections.Counter(total_words)\n",
    "freq_cnt = dict(counter)\n",
    "\n",
    "print(\"Number of unique words = \" + str(len(freq_cnt.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b06afaf7-d301-4300-83b9-955445970900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the word-freq pairs (from the dictionary freq_cnt) in a list, sorted in decreasing order of frequency\n",
    "sorted_freq_cnt = sorted(freq_cnt.items(), reverse=True, key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "436edf48-61d6-4dc7-991d-563489dbc5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common unique words = 5136\n"
     ]
    }
   ],
   "source": [
    "threshold = 10\n",
    "\n",
    "#Filter off those words whose frequency of occurrence in less than threshold\n",
    "sorted_freq_cnt = [x for x in sorted_freq_cnt if x[1]>threshold]\n",
    "# Store these common words in total_words\n",
    "total_words = [x[0] for x in sorted_freq_cnt]\n",
    "\n",
    "print(\"Number of common unique words = \" + str(len(total_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f46b00c-32e2-4152-912c-2afd81512dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training and testing image names\n",
    "\n",
    "train_file_data = \"\"\n",
    "test_file_data = \"\"\n",
    "\n",
    "with open (\"C:/Users/ASUS/Desktop/image caption generator/data/textFiles/flickr30k_train.txt\", 'r') as file:\n",
    "    train_file_data = file.read()\n",
    "\n",
    "with open (\"C:/Users/ASUS/Desktop/image caption generator/data/textFiles/flickr30k_test.txt\", 'r') as file:\n",
    "    test_file_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71a1d29f-1b8e-438f-a3f7-8582439bd80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a list of train and test images\n",
    "train_data = [img_file_name for img_file_name in train_file_data.split(\"\\n\")[:-1]]\n",
    "test_data = [img_file_name for img_file_name in test_file_data.split(\"\\n\")[:-1]]\n",
    "\n",
    "# Obtain image ID from image file name\n",
    "train_data = [image.split(\".\")[0] for image in train_data]\n",
    "test_data = [image.split(\".\")[0] for image in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7cb7422-f68c-4549-a1e4-52bbcda9e382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1000092795', '10002456', '1000268201', '1000344755', '1000366164']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23c2f7be-b27b-41ed-a464-86243b5ba700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each imageID in train_data, store its captions in a dictionary \n",
    "\n",
    "train_content = {}\n",
    "\n",
    "for imageID in train_data:\n",
    "    train_content[imageID] = []\n",
    "    for caption in content[imageID]:\n",
    "        # Add a start sequence token in the beginning and an end sequence token at the end\n",
    "        cap_to_append = \"startseq \" + caption + \" endseq\"\n",
    "        train_content[imageID].append(cap_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8be91967-de2c-4406-a901-e525808d8a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq a man with reflective safety clothes and ear protection drives a john deere tractor on a road  endseq',\n",
       " 'startseq john deere tractors cruises down a street while the driver wears easy to see clothing  endseq',\n",
       " 'startseq a man in a neon green and orange uniform is driving on a green tractor  endseq',\n",
       " 'startseq a man in a tractor wearing headphones driving down a paved street  endseq',\n",
       " 'startseq a man driving a john deere tractor on a main road in the country  endseq']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content['1001896054']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756aa456-5fae-4c45-8501-ada0652312c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3d6de837-58a5-46df-89b0-0daa49007f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet50\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResNet50, preprocess_input, decode_predictions\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet50(weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "\n",
    "model = ResNet50(weights = 'imagenet', input_shape = (224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b4ffe598-97bd-4875-acef-c23c54bd1bc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e7fa2-00fb-4bb6-8b56-f2a7f58b3e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60b4fcb-73bb-4ca7-8abe-30042ae54f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "model_new = Model (model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139aeb96-ddb3-4ce8-b9ca-a6b8d59bab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image (img):\n",
    "    img = image.load_img(img, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "\n",
    "    # Convert 3D tensor to a 4D tendor\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    #Normalize image accoring to ResNet50 requirement\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c2b2d-5e33-4be3-81ca-8b63f3e527de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "try:\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "except NameError:\n",
    "    pass\n",
    "img = preprocess_image(\"data/flickr30k_images/4376178.jpg\")\n",
    "print(img.shape)\n",
    "plt.imshow(img[0])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177cb127-a13d-42b6-841e-0279677e0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper function, which inputs an image and returns its encoding (feature vector)\n",
    "def encode_image (img):\n",
    "    img = preprocess_image(img)\n",
    "    feature_vector = model_new.predict(img)\n",
    "\n",
    "    feature_vector = feature_vector.reshape((-1,))\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e4826-7c59-49ec-a208-088cbc281921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "# File to save the encodings\n",
    "encoding_file = \"train_encodings.pkl\"\n",
    "\n",
    "# Check if saved encodings already exist\n",
    "if os.path.exists(encoding_file):\n",
    "    print(\"Loading saved encodings...\")\n",
    "    with open(encoding_file, \"rb\") as f:\n",
    "        train_encoding = pickle.load(f)\n",
    "else:\n",
    "    print(\"Encoding images...\")\n",
    "    train_encoding = {}\n",
    "    start_time = time()\n",
    "\n",
    "    for imageID in tqdm(train_data, desc=\"Encoding images\"):\n",
    "        image_path = os.path.join(\"data\", \"flickr30k_images\", imageID + \".jpg\")\n",
    "        train_encoding[imageID] = encode_image(image_path)\n",
    "\n",
    "    end_time = time()\n",
    "    print(\"Encoding complete. Total time taken:\", end_time - start_time, \"sec\")\n",
    "\n",
    "    # Save to file\n",
    "    with open(encoding_file, \"wb\") as f:\n",
    "        pickle.dump(train_encoding, f)\n",
    "    print(f\"Encodings saved to '{encoding_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33697ca-3fb4-4083-bc9a-814805d15408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "# File to save the test encodings\n",
    "test_encoding_file = \"test_encodings.pkl\"\n",
    "\n",
    "# Check if saved encodings already exist\n",
    "if os.path.exists(test_encoding_file):\n",
    "    print(\"Loading saved test encodings...\")\n",
    "    with open(test_encoding_file, \"rb\") as f:\n",
    "        test_encoding = pickle.load(f)\n",
    "else:\n",
    "    print(\"Encoding test images...\")\n",
    "    test_encoding = {}\n",
    "    start_time = time()\n",
    "\n",
    "    for imageID in tqdm(test_data, desc=\"Encoding test images\"):\n",
    "        image_path = os.path.join(\"data\", \"flickr30k_images\", imageID + \".jpg\")\n",
    "        test_encoding[imageID] = encode_image(image_path)\n",
    "\n",
    "    end_time = time()\n",
    "    print(\"Encoding complete. Total time taken:\", round(end_time - start_time, 2), \"s\")\n",
    "\n",
    "    # Save encodings to file\n",
    "    with open(test_encoding_file, \"wb\") as f:\n",
    "        pickle.dump(test_encoding, f)\n",
    "    print(f\"Test encodings saved to '{test_encoding_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f5d10f-bf84-4de2-8a4b-0f43417ff3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c1e7f-717c-4abb-8849-3430057984af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the word-to-index and index-to-word mappings\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "\n",
    "for i, word in enumerate(total_words):\n",
    "    word_to_index[word] = i+1\n",
    "    index_to_word[i+1] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3507cc49-d2ea-4742-be26-1ede326086ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75890dd-6baf-47fb-9543-4f514715d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_to_word[5])\n",
    "print(word_to_index['is'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79e1a0-071e-423e-8d72-3c199c7aeafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add startseq and endseq also to the mappings\n",
    "index_to_word[5137] = 'startseq'\n",
    "word_to_index['startseq'] = 5137\n",
    "\n",
    "index_to_word[5138] = 'endseq'\n",
    "word_to_index['endseq'] = 5138\n",
    "\n",
    "VOCAB_SIZE = len(word_to_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf9adf-f722-4149-8b89-b96646a0bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b7bbd9-062c-48ed-b585-ebf208dc8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/textFiles/word_to_idx.pkl\", \"wb\") as file:\n",
    "    pickle.dump(word_to_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405e399-3ac1-4d19-9cf5-21e9e97ea390",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/textFiles/idx_to_word.pkl\", \"wb\") as file:\n",
    "    pickle.dump(index_to_word, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66036ff5-9a8f-4f62-a89e-2509662ac41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum length of a caption\n",
    "max_len = 0\n",
    "\n",
    "for cap_list in train_content.keys():\n",
    "    for caption in train_content[cap_list]:\n",
    "        max_len = max(max_len, len(caption.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1657fba-d37c-48c0-988c-9858bb3cf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be14619-052d-461c-bf0f-9f92ca0f7048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Glove word Embeddings\n",
    "# This contains 50-dimensional embeddings for 6 Billion English words\n",
    "file = open(\"glove.6B.50d.txt\",encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ad80a-b1a4-4681-9b55-620b91aeb612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from word to embedding\n",
    "embeddings_index = {} # empty dictionary\n",
    "\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "\n",
    "    word = values[0]\n",
    "    coefs = np.array (values[1:], dtype='float')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634219e8-9be2-47e9-8192-53bcbf3edf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59406e10-b507-4242-a68e-76791c99d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, embedding_dim))\n",
    "\n",
    "for word, i in word_to_index.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f7cc9-2762-4cc2-8c36-bc5f45d0d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce2b4d5-65a2-4b39-b393-b8454f5dbb63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06604299-1e34-4273-9d69-d6c21696eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM\n",
    "\n",
    "#Convert feature vector of image to smaller vector\n",
    "\n",
    "#Output of ResNet goes into following input layer \n",
    "inp_img_features = Input(shape=(2048,))\n",
    "\n",
    "inp_img1 = Dropout(0.3)(inp_img_features)\n",
    "inp_img2 = Dense(256, activation='relu')(inp_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bfb252-74c2-467c-a567-700f97567d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now take Captions as input\n",
    "\n",
    "#Actual input size will be (batch_size x max_length_of_caption)\n",
    "#But here we specify only for one example\n",
    "inp_cap = Input(shape=(max_len,))\n",
    "inp_cap1 = Embedding(input_dim=VOCAB_SIZE, output_dim=50, mask_zero=True)(inp_cap)\n",
    "inp_cap2 = Dropout(0.3)(inp_cap1)\n",
    "#inp_cap3 = LSTM(256, use_cudnn=False)(inp_cap2)\n",
    "\n",
    "inp_cap3 = LSTM(256)(inp_cap2)\n",
    "inp_cap4 = Dropout(0.3)(inp_cap3)\n",
    "# inp_cap3 captures the entire sentence that has been generated till now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b8747-059e-4942-b125-e48be02d83fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import add\n",
    "\n",
    "# Decode the inputs\n",
    "\n",
    "# So, an image (224x224x3) goes through ResNet50\n",
    "# Then as 2048 dimensional it goes through the above earlier architecture\n",
    "# The final output is inp_img2 (256 dimensional) which now goes through the Decoder \n",
    "\n",
    "# Similarly for the captions which initially have shape (batch_size x max_len)\n",
    "# Then after passing through Embedding layer comes out as (batch_size x max_len x 50(embedding_size)))\n",
    "# Then it passes through the above LSTM layer and comes out as inp_cap3 (a 256 dimensional vector)\n",
    "\n",
    "# Add the two above tensors\n",
    "decoder1 = add([inp_img2, inp_cap3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(VOCAB_SIZE, activation='softmax')(decoder2)\n",
    "\n",
    "# Combined model\n",
    "model = Model (inputs=[inp_img_features, inp_cap], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8da51f-812a-48a3-82e9-2fb14e315534",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44014f01-50aa-4a99-b5da-5e48afe2f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe4ab0-0b6c-4384-9932-e69f3cb6a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8a53f-4532-4399-ab5e-2b072401ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de8d19a-7e45-4dfc-ac38-094b2e193270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e805923-14bc-4fa4-8a4f-ae2a01773ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "class ImageCaptioningGenerator(Sequence):\n",
    "    def __init__(self, train_content, train_encoding, word_to_index, max_len, batch_size, vocab_size, **kwargs):\n",
    "        super().__init__(**kwargs)  # ✅ this is the fix!\n",
    "        self.train_content = list(train_content.items())\n",
    "        self.train_encoding = train_encoding\n",
    "        self.word_to_index = word_to_index\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.train_content) / float(self.batch_size)))\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X1, X2, y = [], [], []\n",
    "        start = idx * self.batch_size\n",
    "        end = start + self.batch_size\n",
    "        batch_items = self.train_content[start:end]\n",
    "\n",
    "        for imageID, cap_list in batch_items:\n",
    "            image = self.train_encoding[imageID]\n",
    "            for caption in cap_list:\n",
    "                idx_seq = [self.word_to_index[word] for word in caption.split() if word in self.word_to_index]\n",
    "    \n",
    "                for i in range(1, len(idx_seq)):\n",
    "                    xi = idx_seq[0:i]\n",
    "                    yi = idx_seq[i]\n",
    "    \n",
    "                    xi = pad_sequences([xi], maxlen=self.max_len, value=0, padding='post')[0]\n",
    "                    yi = to_categorical([yi], num_classes=self.vocab_size)[0]\n",
    "    \n",
    "                    #X1.append(image)\n",
    "                    X1.append(np.reshape(image, (2048,)))  # Ensure it's the right shape\n",
    "\n",
    "                    X2.append(xi)\n",
    "                    y.append(yi)\n",
    "\n",
    "        return (np.array(X1), np.array(X2)), np.array(y)  # ✅ must return a tuple!\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''' def __getitem__(self, idx):\n",
    "            X1, X2, y = [], [], []\n",
    "            start = idx * self.batch_size\n",
    "            end = start + self.batch_size\n",
    "            batch_items = self.train_content[start:end]\n",
    "\n",
    "        for imageID, cap_list in batch_items:\n",
    "            image = self.train_encoding[imageID]\n",
    "            for caption in cap_list:\n",
    "                idx_seq = [self.word_to_index[word] for word in caption.split() if word in self.word_to_index]\n",
    "\n",
    "                for i in range(1, len(idx_seq)):\n",
    "                    xi = idx_seq[0:i]\n",
    "                    yi = idx_seq[i]\n",
    "\n",
    "                    xi = pad_sequences([xi], maxlen=self.max_len, value=0, padding='post')[0]\n",
    "                    yi = to_categorical([yi], num_classes=self.vocab_size)[0]\n",
    "\n",
    "                    X1.append(image)\n",
    "                    X2.append(xi)\n",
    "                    y.append(yi)\n",
    "\n",
    "        return [np.array(X1), np.array(X2)], np.array(y)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c4c717-b61e-4886-903e-aa7662781dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 15\n",
    "batch_size = 5\n",
    "steps = len(train_content)//batch_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "generator = ImageCaptioningGenerator(\n",
    "    train_content=train_content,\n",
    "    train_encoding=train_encoding,\n",
    "    word_to_index=word_to_index,\n",
    "    max_len=max_len,\n",
    "    batch_size=batch_size,\n",
    "    vocab_size=VOCAB_SIZE\n",
    ")\n",
    "\n",
    "for i in range(epochs):\n",
    "    model.fit(generator, epochs=1)\n",
    "    #model.save(f'model_{i}.h5')\n",
    "    model.save(f'model_{i}.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b8f76d-4daf-46b9-a268-4bda30ad2e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa744b5-9716-44be-818b-da2cc5a040bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./model_14.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae77c9-f9c4-4da4-b387-7d70a4d805e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = './data/flickr30k_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5d7cd-1620-4925-9cb6-4741df129268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"./test_encodings.pkl\"\n",
    "print(f\"File size: {os.path.getsize(file_path)} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e6fda-9687-4263-b51d-c0ce17cae5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load\n",
    "with open(\"./test_encodings.pkl\", \"rb\") as encoded_pickle:\n",
    "    encoding_test = load(encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef34878-22ea-4885-b945-96db67eecd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedySearch(photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_len):\n",
    "        sequence = [word_to_index[w] for w in in_text.split() if w in word_to_index]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_len,padding='post')\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = index_to_word[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c86cab-e5bc-4695-a52f-63fa2801516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cfc9b0-4123-427a-901e-888666995f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z = 0\n",
    "z+=1\n",
    "pic = list(encoding_test.keys())[z]\n",
    "image = encoding_test[pic].reshape((1,2048))\n",
    "x=plt.imread(images+pic+'.jpg')\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "print(\"Greedy:\",greedySearch(image))\n",
    "print(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e30eb2-af5f-4214-b025-3b11e6573736",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = {}\n",
    "\n",
    "for imageID in test_data:\n",
    "    test_content[imageID] = []\n",
    "    for caption in content[imageID]:\n",
    "        # Add a start sequence token in the beginning and an end sequence token at the end\n",
    "        cap_to_append = caption\n",
    "        test_content[imageID].append(cap_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60078a24-e26e-4154-ac6c-bfeeed48c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "pic = list(encoding_test.keys())[x]\n",
    "test_content[pic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3257bdd-3172-4fba-b5e2-ba3548512164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "x = 3\n",
    "pic = list(encoding_test.keys())[x]\n",
    "test_content[pic]\n",
    "\n",
    "\n",
    "reference = test_content[pic]\n",
    "#pic = list(encoding_test.keys())[1]\n",
    "print(pic)\n",
    "img = 'twodogs.jpg'\n",
    "e = encoding_test[pic].reshape(1,2048)\n",
    "#image = encoding_test[pic].reshape((1,2048))\n",
    "x=plt.imread(images+pic+'.jpg')\n",
    "#x=plt.imread(img)\n",
    "plt.imshow(x)\n",
    "# plt.show()\n",
    "caption = greedySearch(e)\n",
    "print(\"Greedy Search Caption:\",caption)\n",
    "print()\n",
    "print('Reference 1:',reference)\n",
    "\n",
    "print()\n",
    "print('BLEU-1:', round(sentence_bleu(reference, caption),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43d9f1-bd92-4e25-a480-fd3aefd34e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f79e5-a78c-4e70-8817-24664a0a1c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6711477-9ded-4ddb-99e1-772903cf80e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c4991-01ab-40d4-9383-9b3dee14deb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52270d9-a3d7-4dec-9e3a-8c8d3ba41341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78316e68-684b-475f-b57a-5bed098cf8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb925f62-f7ab-46f4-844d-4deeacf5924e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd4e37-0af0-4b88-b391-8e0ac3a8128c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
